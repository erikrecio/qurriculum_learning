{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sympy.utilities.iterables import multiset_permutations\n",
    "\n",
    "import jax.random\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy.optimize\n",
    "import jaxopt\n",
    "import optax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import pennylane as qml\n",
    "from functools import partial\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "plt.ioff()\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pytz\n",
    "import ast\n",
    "from pypdf import PdfMerger\n",
    "\n",
    "import contextlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Running parameters\n",
    "num_iters = 2000    # Number of training iterations\n",
    "num_runs = 10\n",
    "cl_types = [\"NCL\"]#, \"CL\", \"ACL\", \"SPCL\", \"SPACL\"]     # \"NCL\" - No curriculum, \"CL\" - Curriculum, \"ACL\" - Anti-curriculum, \"SPCL\" - Self paced curriculum, \"SPACL\" - Self paced anti-curriculum\n",
    "\n",
    "# Circuit parameters\n",
    "nqubits = 6         # Num qubits, min 4, always 2**num_layers qubits\n",
    "layers = 15          # Num layers of the variational unitary\n",
    "qcnn_mode = False   # True: All the gates in the same layer share weights. False: Each gates has its own weight.\n",
    "device = \"default.qubit\"\n",
    "\n",
    "# Data hyper-parameters\n",
    "dataset_type = \"haar\"  # \"basis\" or \"haar\"\n",
    "batch_size = 15    # batch training size\n",
    "train_size = 15    # Total states that will be used for training\n",
    "val_size = 50      # Total states that will be used for validation\n",
    "cl_batch_ratios = [0.3, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]  #[0.4, 0.3, 0.2, 0.1]    # [0.1, 0.2, 0.3, 0.4], [0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], [0.3, 0.2, 0.2, 0.2, 0.1]\n",
    "cl_iter_ratios  = [0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]\n",
    "\n",
    "# Optimization parameters\n",
    "optimizer = \"Adam\"  # \"Adam\", \"GradientDescent\", \"BFGS\"\n",
    "loss_type = \"fidelity\" # \"fidelity\"\n",
    "dist_type = \"fro\"  # \"fro\", np.inf, 2, 1\n",
    "initialization = \"gaussian\" # \"gaussian\", \"uniform\"\n",
    "max_weight_init = 2*np.pi  # weight_init goes from 0 to this number. Max = 2*np.pi. Other options = 0.01\n",
    "stepsize = 0.01         # stepsize of the gradient descent.\n",
    "\n",
    "# Constant definitions\n",
    "if qcnn_mode:\n",
    "    nweights = 15*layers\n",
    "else:\n",
    "    nweights = 15*(layers//2*(int(nqubits//2) + int((nqubits-1)//2)) + layers%2*(int(nqubits//2)))\n",
    "dev = qml.device(device, wires=nqubits)\n",
    "\n",
    "cl_batches = []\n",
    "i_batch_size = 0\n",
    "for i in range(len(cl_iter_ratios)):\n",
    "    if i < len(cl_iter_ratios)-1:\n",
    "        i_batch_size += int(cl_batch_ratios[i]*train_size)\n",
    "        i_num_iters = int(cl_iter_ratios[i]*num_iters)\n",
    "    else:\n",
    "        i_batch_size = train_size\n",
    "        i_num_iters = num_iters - len(cl_batches)\n",
    "        \n",
    "    cl_batches += [i_batch_size]*i_num_iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def X(i):\n",
    "    return qml.PauliX(i)\n",
    "\n",
    "def Y(i):\n",
    "    return qml.PauliY(i)\n",
    "\n",
    "def Z(i):\n",
    "    return qml.PauliZ(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamiltonian_unitary():\n",
    "    hamiltonian = sum(X(i) @ X((i+1)) + Y(i) @ Y((i+1)) + 1.5 * Z(i) @ Z((i+1)) for i in range(nqubits-1))\n",
    "    qml.ApproxTimeEvolution(hamiltonian, time=0.5, n=10)\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def hamiltonian_evolution(ini_state):\n",
    "    qml.QubitStateVector(ini_state, wires=range(nqubits))\n",
    "    hamiltonian_unitary()\n",
    "    return qml.state()\n",
    "\n",
    "\n",
    "def get_random_basis_state():\n",
    "    choice_arr = np.array([1]+[0]*(2**nqubits-1))\n",
    "    state = np.random.choice(choice_arr, size=2**nqubits, replace=False)\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_basis_dataset(num_points):\n",
    "    basis_states = []\n",
    "    target_states = []\n",
    "    if num_points > 2**nqubits:\n",
    "        raise ValueError(f\"Error trying to generate {num_points} basis states. You can not generate more than {2**nqubits}.\")\n",
    "    while len(basis_states) < num_points:\n",
    "        ini_state = get_random_basis_state()\n",
    "        if not np.array([(ini_state==state).all() for state in basis_states]).any():  # Checks if ini_state has already been sampled\n",
    "            target_state = hamiltonian_evolution(ini_state)\n",
    "            basis_states.append(ini_state)\n",
    "            target_states.append(target_state)\n",
    "\n",
    "    return np.array(basis_states), np.array(target_states)\n",
    "\n",
    "\n",
    "def generate_all_basis_dataset():\n",
    "    \n",
    "    choice_arr = np.array([1]+[0]*(2**nqubits-1))\n",
    "    all_basis_states = np.array(list(multiset_permutations(choice_arr)))\n",
    "    np.random.shuffle(all_basis_states)\n",
    "\n",
    "    target_states = []\n",
    "    for ini_state in all_basis_states:\n",
    "        target_states.append(hamiltonian_evolution(ini_state))\n",
    "\n",
    "    return all_basis_states, np.array(target_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_haar(N):\n",
    "    \"\"\"Generate a Haar-random matrix using the QR decomposition.\"\"\"\n",
    "    \"\"\"https://pennylane.ai/qml/demos/tutorial_haar_measure/\"\"\"\n",
    "    \n",
    "    A, B = np.random.normal(size=(N, N)), np.random.normal(size=(N, N))\n",
    "    Z = A + 1j * B\n",
    "\n",
    "    Q, R = np.linalg.qr(Z)\n",
    "    Lambda = np.diag([R[i, i] / np.abs(R[i, i]) for i in range(N)])\n",
    "\n",
    "    return np.dot(Q, Lambda)\n",
    "\n",
    "\n",
    "def get_random_haar_state(num_qubits):\n",
    "    qml.QubitUnitary(qr_haar(2**num_qubits), wires=range(num_qubits))\n",
    "    return qml.state()\n",
    "\n",
    "\n",
    "def generate_haar_dataset(num_points):\n",
    "    global get_random_haar_state\n",
    "    dev_haar = qml.device(device, wires=nqubits)\n",
    "    get_random_haar_state = qml.QNode(get_random_haar_state, dev_haar)\n",
    "\n",
    "    haar_states = []\n",
    "    target_states = []\n",
    "    while len(haar_states) < num_points:\n",
    "        ini_state = get_random_haar_state(nqubits)\n",
    "        target_state = hamiltonian_evolution(ini_state)\n",
    "        haar_states.append(ini_state)\n",
    "        target_states.append(target_state)\n",
    "\n",
    "    return np.array(haar_states), np.array(target_states)\n",
    "\n",
    "\n",
    "def generate_local_haar_dataset(num_points):\n",
    "    global get_random_haar_state\n",
    "    dev_haar = qml.device(device, wires=1)\n",
    "    get_random_haar_state = qml.QNode(get_random_haar_state, dev_haar)\n",
    "    \n",
    "    haar_states = []\n",
    "    target_states = []\n",
    "    while len(haar_states) < num_points:\n",
    "        ini_state = 1\n",
    "        for _ in range(nqubits):\n",
    "            ini_state = np.tensordot(ini_state, get_random_haar_state(1), axes=0).flatten()\n",
    "        target_state = hamiltonian_evolution(ini_state)\n",
    "        haar_states.append(ini_state)\n",
    "        target_states.append(target_state)\n",
    "\n",
    "    return np.array(haar_states), np.array(target_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Quantum Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_unitary_2q(q1, q2, weights):\n",
    "    qml.U3(wires=q1, theta=weights[0], phi=weights[1], delta=weights[2])\n",
    "    qml.U3(wires=q1, theta=weights[3], phi=weights[4], delta=weights[5])\n",
    "    qml.CNOT(wires=[q2, q1])\n",
    "    qml.RZ(wires=q1, phi=weights[6])\n",
    "    qml.RY(wires=q2, phi=weights[7])\n",
    "    qml.CNOT(wires=[q1, q2])\n",
    "    qml.RY(wires=q2, phi=weights[8])\n",
    "    qml.CNOT(wires=[q2, q1])\n",
    "    qml.U3(wires=q1, theta=weights[9], phi=weights[10], delta=weights[11])\n",
    "    qml.U3(wires=q1, theta=weights[12], phi=weights[13], delta=weights[14])\n",
    "\n",
    "\n",
    "\n",
    "def variational_unitary(weights):\n",
    "    k = 0\n",
    "    for _ in range(layers//2):\n",
    "        \n",
    "        i = 0\n",
    "        while 2*i+1 < nqubits:\n",
    "            general_unitary_2q(2*i, 2*i+1, weights[k:k+15])\n",
    "            k += 15 if not qcnn_mode else 0\n",
    "            i += 1\n",
    "        k += 0 if not qcnn_mode else 15\n",
    "\n",
    "        i = 0\n",
    "        while 2*i+2 < nqubits:\n",
    "            general_unitary_2q(2*i+1, 2*i+2, weights[k:k+15])\n",
    "            k += 15 if not qcnn_mode else 0\n",
    "            i += 1\n",
    "        k += 0 if not qcnn_mode else 15\n",
    "    \n",
    "    if layers % 2 != 0:\n",
    "        i = 0\n",
    "        while 2*i+1 < nqubits:\n",
    "            general_unitary_2q(2*i, 2*i+1, weights[k:k+15])\n",
    "            k += 15 if not qcnn_mode else 0\n",
    "            i += 1\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "@qml.qnode(dev, interface=\"jax\", diff_method=\"best\")\n",
    "def variational_circuit(weights, state_ini):\n",
    "    qml.QubitStateVector(state_ini, wires=range(nqubits))\n",
    "    variational_unitary(weights)\n",
    "    return qml.state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def single_loss(weights, ini_state, target_state):\n",
    "    \n",
    "    out_state = variational_circuit(weights, ini_state)\n",
    "\n",
    "    if loss_type == \"fidelity\":\n",
    "        cost = 1 - jnp.abs(jnp.vdot(out_state, target_state))**2\n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss(weights, ini_states, target_states):\n",
    "    costs = jax.vmap(single_loss, in_axes=[None, 0, 0])(weights, ini_states, target_states)\n",
    "    return costs.sum()/len(ini_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def unitary_distance(weights):\n",
    "    unitary_matrix = qml.matrix(hamiltonian_unitary)()\n",
    "    variational_matrix = qml.matrix(variational_unitary)(weights)\n",
    "    return jnp.linalg.norm(variational_matrix - unitary_matrix, ord=dist_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_multi_image(filename):\n",
    "    pp = PdfPages(filename)\n",
    "    fig_nums = plt.get_fignums()\n",
    "    figs = [plt.figure(n) for n in fig_nums]\n",
    "    for fig in figs:\n",
    "        fig.savefig(pp, bbox_inches='tight', format='pdf')\n",
    "    pp.close()\n",
    "\n",
    "\n",
    "def close_all_figures():\n",
    "    fig_nums = plt.get_fignums()\n",
    "    for n in fig_nums:\n",
    "        plt.figure(n)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def save_plots(time_now,\n",
    "               folder_name,\n",
    "               file_name,\n",
    "               plot_run,\n",
    "               uni_dists,\n",
    "               losses_train,\n",
    "               losses_val,\n",
    "               losses_val_all_states\n",
    "              ):\n",
    "\n",
    "\n",
    "    fig, axis = plt.subplots(1,2)\n",
    "    fig.set_figheight(6.5)\n",
    "    fig.set_figwidth(15)\n",
    "    fig.tight_layout(w_pad=6)  # otherwise the right y-label is slightly clipped # rect=(1,1,5,1)\n",
    "\n",
    "    # ---------------------------------------------------------------------- #\n",
    "    # -------------------- Loss and accuracy figure ------------------------ #\n",
    "    # ---------------------------------------------------------------------- #\n",
    "\n",
    "    iterations = range(1, num_iters+1)\n",
    "\n",
    "    color1 = 'darkred'\n",
    "    axis[0].set_xlabel('Iterations')\n",
    "    axis[0].set_ylabel('Unitary dist.', color=color1)\n",
    "    axis[0].plot(iterations, uni_dists, label=\"Unitary dist.\", color=color1)\n",
    "    axis[0].tick_params(axis='y', labelcolor=color1)\n",
    "    axis[0].set_ylim(bottom=0)\n",
    "\n",
    "    ax2 = axis[0].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color2 = 'darkblue'\n",
    "    ax2.set_ylabel('Loss', color=color2)  # we already handled the x-label with axis[0]\n",
    "    ax2.plot(iterations, losses_train, label=\"Train Loss\", color=color2)\n",
    "    ax2.plot(iterations, losses_val, '-.', label=\"Val. Loss\", color=color2)\n",
    "    ax2.tick_params(axis='y', labelcolor=color2)\n",
    "    ax2.set_yscale(\"log\")\n",
    "    ax2.set_ylim(bottom=0.0, top=1.0)\n",
    "\n",
    "    # plt.legend()\n",
    "    axis[0].set_title(f\"Unitary dist. and Loss - Run {plot_run}\")\n",
    "\n",
    "    # -------------------------------------------------------------------- #\n",
    "    # -------------------- Loss validation figure ------------------------ #\n",
    "    # -------------------------------------------------------------------- #\n",
    "\n",
    "    iterations = range(1, num_iters+1)\n",
    "\n",
    "    axis[1].yaxis.set_label_position(\"right\")\n",
    "    axis[1].yaxis.tick_right()\n",
    "\n",
    "    color2 = 'darkblue'\n",
    "    axis[1].set_ylabel('Loss', color=color2)  # we already handled the x-label with axis[0]\n",
    "\n",
    "    for loss in losses_val_all_states:\n",
    "        axis[1].plot(iterations, loss, color=color2)\n",
    "    \n",
    "    axis[1].tick_params(axis='y', labelcolor=color2)\n",
    "    axis[1].set_yscale(\"log\")\n",
    "    axis[1].set_ylim(bottom=0.0, top=1.0)\n",
    "\n",
    "    # plt.legend()\n",
    "    axis[1].set_title(f\"Loss all validation states - Run {plot_run}\")\n",
    "\n",
    "\n",
    "    \n",
    "    # ---------------------------------------------------------------------- #\n",
    "    # --------------------------- Save plots ------------------------------- #\n",
    "    # ---------------------------------------------------------------------- #\n",
    "\n",
    "    plots_pdf_name = f\"{folder_name}/{time_now} - Plots - {file_name}.pdf\"\n",
    "    \n",
    "    \n",
    "    # If the file doesn't exist we save it. If it does, we merge it.\n",
    "    if not os.path.isfile(plots_pdf_name):\n",
    "        save_multi_image(plots_pdf_name)\n",
    "    \n",
    "    else:\n",
    "        save_multi_image(plots_pdf_name + \"2\")\n",
    "        # Merge the new plot with the rest and delete the last file\n",
    "        merger = PdfMerger()\n",
    "        merger.append(plots_pdf_name)\n",
    "        merger.append(plots_pdf_name + \"2\")\n",
    "        merger.write(plots_pdf_name)\n",
    "        merger.close()\n",
    "        os.remove(plots_pdf_name + \"2\")\n",
    "    \n",
    "    close_all_figures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hyperparameters(time_now, folder_name, file_name):\n",
    "    \n",
    "    # --------------- Hyperparameters -----------------#\n",
    "    hyperparameters = {}\n",
    "    hyperparameters[\"num_iters\"] = [num_iters]\n",
    "    hyperparameters[\"num_runs\"] = [num_runs]\n",
    "    hyperparameters[\"cl_types\"] = [cl_types]\n",
    "    hyperparameters[\"nqubits\"] = [nqubits]\n",
    "    hyperparameters[\"layers\"] = [layers]\n",
    "    hyperparameters[\"qcnn_mode\"] = qcnn_mode\n",
    "    hyperparameters[\"device\"] = device\n",
    "    hyperparameters[\"dataset_type\"] = dataset_type\n",
    "    hyperparameters[\"batch_size\"] = [batch_size]\n",
    "    hyperparameters[\"train_size\"] = [train_size]\n",
    "    hyperparameters[\"val_size\"] = val_size\n",
    "    hyperparameters[\"cl_batch_ratios\"] = [cl_batch_ratios]\n",
    "    hyperparameters[\"cl_iter_ratios\"] = [cl_iter_ratios]\n",
    "    hyperparameters[\"optimizer\"] = [optimizer]\n",
    "    hyperparameters[\"loss_type\"] = [loss_type]\n",
    "    hyperparameters[\"dist_type\"] = [dist_type]\n",
    "    hyperparameters[\"initialization\"] = [initialization]\n",
    "    hyperparameters[\"max_weight_init\"] = [max_weight_init]\n",
    "    hyperparameters[\"stepsize\"] = [stepsize]\n",
    "    hyperparameters[\"key\"] = [time_now]\n",
    "\n",
    "    hyperparameters = pd.DataFrame(hyperparameters)\n",
    "\n",
    "    hyperparameters_file_name = f\"{folder_name}/{time_now} - Hyperparameters{file_name}.csv\"\n",
    "    hyperparameters.to_csv(hyperparameters_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(time_now,\n",
    "              folder_name,\n",
    "              run,\n",
    "              weights,\n",
    "              losses_train,\n",
    "              losses_val,\n",
    "              uni_dists,\n",
    "              run_time,\n",
    "              cl,\n",
    "              losses_val_all_states\n",
    "              ):\n",
    "    \n",
    "    # -------------------- Total Data -------------------- #\n",
    "    data = {}\n",
    "    data[\"run\"] = run\n",
    "    \n",
    "    it_min = np.argmin(np.array(uni_dists))\n",
    "    uni_dist_min = uni_dists[it_min]\n",
    "    uni_dist_last = uni_dists[num_iters-1]\n",
    "    \n",
    "    data[\"it_min\"] = it_min\n",
    "    data[\"uni_dist_min\"] = uni_dist_min\n",
    "    data[\"uni_dist_last\"] = uni_dist_last\n",
    "    data[\"run_time\"] = run_time\n",
    "    \n",
    "    data[\"weights\"] = [weights]\n",
    "    data[\"losses_train\"] = [losses_train]\n",
    "    data[\"losses_val\"] = [losses_val]\n",
    "    data[\"uni_dists\"] = [uni_dists]\n",
    "    \n",
    "\n",
    "    data = pd.DataFrame(data)\n",
    "    \n",
    "    data_file_name = f\"{folder_name}/{time_now} - Data - {cl}.csv\"\n",
    "    data.to_csv(data_file_name, index=False, mode='a', header = not os.path.exists(data_file_name))\n",
    "    \n",
    "    \n",
    "    # ------------------- Results ------------------- #\n",
    "    \n",
    "    read_data = pd.read_csv(data_file_name,\n",
    "                     usecols=[\"it_min\",\n",
    "                              \"uni_dist_min\",\n",
    "                              \"uni_dists\"],\n",
    "                     converters={\"uni_dists\":ast.literal_eval})\n",
    "    \n",
    "    total_it_min= read_data[\"it_min\"]\n",
    "    total_uni_dist_min = read_data[\"uni_dist_min\"]\n",
    "    total_uni_dists = read_data[\"uni_dists\"].tolist()\n",
    "    \n",
    "    best_run_min = total_uni_dist_min.argmax()\n",
    "    best_it_min = total_it_min[best_run_min]\n",
    "    avg_uni_dist_min = total_uni_dist_min.mean()\n",
    "    \n",
    "    best_run_last = np.argmax(np.array(total_uni_dists)[:,num_iters-1])\n",
    "    avg_uni_dist_last = np.mean(np.array(total_uni_dists)[:,num_iters-1])\n",
    "\n",
    "    results = {}\n",
    "    results[\"type_cl\"] = [cl]\n",
    "    results[\"num_runs\"] = [run+1]\n",
    "    results[\"best_run_min\"] = [best_run_min]\n",
    "    results[\"best_run_last\"] = [best_run_last]\n",
    "    results[\"best_it_min\"] = [best_it_min]\n",
    "    results[\"best_it_last\"] = [num_iters-1]\n",
    "    results[\"best_uni_dist_min\"] = [total_uni_dists[best_run_min][best_it_min]]\n",
    "    results[\"best_uni_dist_last\"] = [total_uni_dists[best_run_last][num_iters-1]]\n",
    "    results[\"avg_uni_dist_min\"] = [avg_uni_dist_min]\n",
    "    results[\"avg_uni_dist_last\"] = [avg_uni_dist_last]\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    results_file_name = f\"{folder_name}/{time_now} - Results.csv\"\n",
    "    \n",
    "    # If file exists, we update the info\n",
    "    if os.path.exists(results_file_name):\n",
    "        read_results = pd.read_csv(results_file_name)\n",
    "        row_index = read_results.loc[read_results[\"type_cl\"] == cl].index\n",
    "        \n",
    "        if row_index.shape != (0,):\n",
    "            read_results.drop(labels=row_index[0], axis=0, inplace=True) # we delete the line if it already exists\n",
    "            \n",
    "        results = pd.concat([read_results, results], ignore_index=True)\n",
    "    \n",
    "    results.to_csv(results_file_name, index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ------------------- Plots ------------------- #\n",
    "    save_plots(time_now,\n",
    "               folder_name,\n",
    "               cl,\n",
    "               run,\n",
    "               uni_dists,\n",
    "               losses_train,\n",
    "               losses_val,\n",
    "               losses_val_all_states\n",
    "              )\n",
    "    \n",
    "    if cl == \"NCL\":\n",
    "        cl_str = \"NCL  \"\n",
    "    elif cl==\"CL\":\n",
    "        cl_str = \"CL   \"\n",
    "    elif cl==\"ACL\":\n",
    "        cl_str = \"ACL  \"\n",
    "    elif cl==\"SPCL\":\n",
    "        cl_str = \"SPCL \"\n",
    "    elif cl==\"SPACL\":\n",
    "        cl_str = \"SPACL\"\n",
    "        \n",
    "    print(\n",
    "        f\" {cl_str} |\"\n",
    "        f\" {run:3d} |\"\n",
    "        f\" {it_min:4d}/{num_iters-1:4d} |\"\n",
    "        f\"  {uni_dists[it_min]:0.0f}/{uni_dists[num_iters-1]:0.0f}  |\"\n",
    "        f\" {run_time:0.0f}\"\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_average_plots(time_now, folder_name):\n",
    "        \n",
    "        transparency = 0.1\n",
    "        \n",
    "        if cl_types == [\"NCL\", \"CL\", \"ACL\", \"SPCL\", \"SPACL\"]:\n",
    "                arr_file_names = [[\"NCL\",\"CL\",\"ACL\"], [\"SPCL\",\"SPACL\"]]\n",
    "        else:\n",
    "                arr_file_names = [cl_types]\n",
    "\n",
    "        for file_names in arr_file_names:\n",
    "\n",
    "                fig, axis = plt.subplots(1,len(file_names))\n",
    "                if len(file_names) == 1:\n",
    "                        axis = [axis]\n",
    "                        fig.tight_layout(rect=(0,0,0,0))\n",
    "                else:\n",
    "                        fig.set_figheight(6)\n",
    "                        fig.set_figwidth(7*len(file_names))\n",
    "                        fig.tight_layout(pad=4, w_pad=7)\n",
    "\n",
    "                i = 0\n",
    "                for file_name in file_names:\n",
    "\n",
    "                        data_file_name = f\"{folder_name}/{time_now} - Data - {file_name}.csv\"\n",
    "\n",
    "                        # Read the saved data #####################\n",
    "                        read_data = pd.read_csv(data_file_name,\n",
    "                                                usecols=[\"losses_train\",\n",
    "                                                         \"losses_val\",\n",
    "                                                         \"uni_dists\"],\n",
    "                                                converters={\"losses_train\":ast.literal_eval,\n",
    "                                                            \"losses_val\":ast.literal_eval,\n",
    "                                                            \"uni_dists\":ast.literal_eval})\n",
    "\n",
    "                        all_runs_losses_train = list(map(np.array, read_data[\"losses_train\"]))\n",
    "                        all_runs_losses_val = list(map(np.array, read_data[\"losses_val\"]))\n",
    "                        all_runs_uni_dists = list(map(np.array, read_data[\"uni_dists\"]))\n",
    "\n",
    "                        # We take the averages\n",
    "                        losses_train = sum(all_runs_losses_train)/num_runs\n",
    "                        losses_val = sum(all_runs_losses_val)/num_runs\n",
    "                        uni_dists = sum(all_runs_uni_dists)/num_runs\n",
    "\n",
    "                        iterations = range(1, num_iters+1)\n",
    "\n",
    "                        color1 = 'darkred'\n",
    "                        axis[i].set_xlabel('Iterations')\n",
    "                        axis[i].set_ylabel('Unitary dist.', color=color1)\n",
    "\n",
    "                        axis[i].plot(iterations, uni_dists, label=\"Unitary dist.\", color=color1)\n",
    "                        for uni_dist in all_runs_uni_dists:\n",
    "                                axis[i].plot(iterations, uni_dist, alpha=transparency, color=color1)\n",
    "\n",
    "                        axis[i].tick_params(axis='y', labelcolor=color1)\n",
    "                        axis[i].set_ylim(bottom=0)\n",
    "\n",
    "                        ax2 = axis[i].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "                        color2 = 'darkblue'\n",
    "                        ax2.set_ylabel('Loss', color=color2)  # we already handled the x-label with axis[0]\n",
    "\n",
    "                        ax2.plot(iterations, losses_train, label=\"Train Loss\", color=color2)\n",
    "                        for loss_train in all_runs_losses_train:\n",
    "                                ax2.plot(iterations, loss_train, alpha=transparency, color=color2)\n",
    "\n",
    "                        ax2.plot(iterations, losses_val, '-.', label=\"Val. Loss\", color=color2)\n",
    "                        for loss_val in all_runs_losses_val:\n",
    "                                ax2.plot(iterations, loss_val, '-.', alpha=transparency, color=color2)\n",
    "\n",
    "                        ax2.tick_params(axis='y', labelcolor=color2)\n",
    "                        ax2.set_yscale(\"log\")\n",
    "                        ax2.set_ylim(bottom=0.0, top=1.0)\n",
    "\n",
    "                        # fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "                        # plt.legend()\n",
    "                        axis[i].set_title(f\"Unitary dist. and Loss - Average {file_name} - ({round(uni_dists[num_iters-1],3)})\")\n",
    "                        \n",
    "                        i += 1\n",
    "                        \n",
    "        plots_pdf_name = f\"{folder_name}/{time_now} - Average plots.pdf\"\n",
    "        save_multi_image(plots_pdf_name)\n",
    "        close_all_figures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sort_states(w, ini_states, target_states, ascending):    \n",
    "    scores = jax.vmap(single_loss, in_axes=[None, 0, 0])(jnp.array(w), jnp.array(ini_states), jnp.array(target_states))\n",
    "    \n",
    "    p = jnp.where(ascending, scores.argsort(), scores.argsort()[::-1])\n",
    "    \n",
    "    return ini_states[p], target_states[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_qcnn(train_states, train_target_states, val_states, val_target_states, opt, cl):\n",
    "    \n",
    "    if initialization == \"uniform\":\n",
    "        weights_init = np.random.uniform(0, max_weight_init, nweights)\n",
    "    elif initialization == \"gaussian\":\n",
    "        weights_init = np.random.normal(0, 1/np.sqrt(nqubits), nweights)\n",
    "        \n",
    "    #Initiaize variables\n",
    "    weights = []\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    uni_dists = []\n",
    "    losses_val_all_states = []\n",
    "\n",
    "    w = weights_init\n",
    "    state = opt.init_state(weights_init, train_states[:2], train_target_states[:2])\n",
    "    \n",
    "    for it in range(num_iters):\n",
    "        \n",
    "        # For self paced learning, we sort the datapoints at every iteration\n",
    "        if cl in [\"SPCL\", \"SPACL\"]:\n",
    "            ascending = True if cl == \"SPCL\" else False\n",
    "            train_states, train_target_states = sort_states(w, train_states, train_target_states, ascending)\n",
    "            \n",
    "        # Once they are sorted, we select the first datapoints into the batch lists\n",
    "        if cl in [\"CL\", \"ACL\", \"SPCL\", \"SPACL\"]:\n",
    "            train_states_batch = train_states[:cl_batches[it]]\n",
    "            train_target_states_batch = train_target_states[:cl_batches[it]]\n",
    "        \n",
    "        elif cl == \"NCL\":\n",
    "            batch_index = np.random.default_rng().choice(len(train_states), size=batch_size, replace=False)\n",
    "            \n",
    "            train_states_batch = train_states[batch_index]\n",
    "            train_target_states_batch = train_target_states[batch_index]\n",
    "\n",
    "            \n",
    "        # Update the weights by one optimizer step\n",
    "        with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "            w, state = opt.update(w, state, train_states_batch, train_target_states_batch)\n",
    "        \n",
    "        if optimizer == \"GradientDescent\":\n",
    "            l_train = loss(w, train_states_batch, train_target_states_batch)\n",
    "        else:\n",
    "            l_train = state.value\n",
    "        \n",
    "        l_val = loss(w, val_states, val_target_states)\n",
    "\n",
    "        # Compute difference between variational unitary and target unitary from hamiltonian evolution\n",
    "        uni_dist = unitary_distance(w)\n",
    "        \n",
    "        weights.append(w.tolist())\n",
    "        losses_train.append(float(l_train))\n",
    "        losses_val.append(float(l_val))\n",
    "        uni_dists.append(float(uni_dist))\n",
    "\n",
    "        loss_val_all_states = jax.vmap(single_loss, in_axes=[None, 0, 0])(jnp.array(w), jnp.array(val_states), jnp.array(val_target_states))\n",
    "        losses_val_all_states.append(np.array(loss_val_all_states))\n",
    "\n",
    "    losses_val_all_states = np.array(losses_val_all_states).transpose()\n",
    "\n",
    "    return weights, losses_train, losses_val, uni_dists, losses_val_all_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # with jax.profiler.trace(\"/tmp/jax-trace\", create_perfetto_link=True):\n",
    "\n",
    "    time_now = datetime.now(pytz.timezone('Europe/Andorra')).strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "\n",
    "    folder_name = f\"Results/{nqubits}q - {num_iters:} iters/\"\n",
    "    if not os.path.isdir(f'{folder_name}'):\n",
    "        os.makedirs(f'{folder_name}')\n",
    "\n",
    "    save_hyperparameters(time_now, folder_name, file_name=\"\")\n",
    "\n",
    "    # t = time.time()\n",
    "    # key = jax.random.PRNGKey(int((t-int(t))*10**10))  # We start with a random (depending on time) seed for the jax keys\n",
    "\n",
    "    # choose variational classifier\n",
    "    if optimizer == \"GradientDescent\":\n",
    "        opt = jaxopt.GradientDescent(loss, stepsize=stepsize, verbose=False, jit=True)\n",
    "    elif optimizer == \"Adam\":\n",
    "        opt = jaxopt.OptaxSolver(loss, optax.adam(stepsize), verbose=False, jit=True)\n",
    "    elif optimizer == \"BFGS\":\n",
    "        opt = jaxopt.BFGS(loss, verbose=False, jit=True)\n",
    "    \n",
    "    \n",
    "    for run in range (num_runs):\n",
    "        \n",
    "        # -------------------------------------------------------------- #\n",
    "        # ------------------- Generate ground states ------------------- #\n",
    "        # -------------------------------------------------------------- #\n",
    "        \n",
    "        print(\"Generating dataset...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        if dataset_type == \"basis\":\n",
    "            basis_states, target_states = generate_all_basis_dataset()\n",
    "        elif dataset_type == \"haar\":\n",
    "            basis_states, target_states = generate_local_haar_dataset(train_size + val_size)\n",
    "\n",
    "        train_states, train_target_states = basis_states[:train_size], target_states[:train_size]\n",
    "        val_states, val_target_states = basis_states[train_size:], target_states[train_size:]\n",
    "        \n",
    "        run_time = time.time() - start_time\n",
    "\n",
    "        print(f\"Dataset generated - {run_time:.0f}s\")\n",
    "        print()\n",
    "        print(\"Max train / Last run\")\n",
    "        print(\"-------------------------------------------\")\n",
    "        print(\"  CL   | Run |   Iter    |Acc train| Time  \")\n",
    "        print(\"-------------------------------------------\")\n",
    "        \n",
    "        \n",
    "        for cl in cl_types:\n",
    "            # ----------------------------------------------------------------------------------------------- #\n",
    "            # ------------------------ Sort training gs by their score if curriculum ------------------------ #\n",
    "            # ----------------------------------------------------------------------------------------------- #\n",
    "\n",
    "            if cl in [\"CL\", \"ACL\"]:\n",
    "                score_it = num_iters-1\n",
    "                ascending = True if cl == \"CL\" else False\n",
    "                train_states, train_target_states = sort_states(weights_ncl[score_it], train_states, train_target_states, ascending)\n",
    "\n",
    "            # ------------------------------------------------------------ #\n",
    "            # ------------------------ Train QCNN ------------------------ #\n",
    "            # ------------------------------------------------------------ #\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            weights, \\\n",
    "            losses_train, \\\n",
    "            losses_val, \\\n",
    "            uni_dists, \\\n",
    "            losses_val_all_states = train_qcnn(train_states,\n",
    "                                            train_target_states,\n",
    "                                            val_states,\n",
    "                                            val_target_states,\n",
    "                                            opt=opt,\n",
    "                                            cl=cl\n",
    "                                            )\n",
    "\n",
    "            run_time = time.time() - start_time\n",
    "            \n",
    "            if cl == \"NCL\":\n",
    "                weights_ncl = weights\n",
    "\n",
    "            # --------------------------------------------------------- #\n",
    "            # ------------------- Save calculations ------------------- #\n",
    "            # --------------------------------------------------------- #\n",
    "            save_data(time_now,\n",
    "                      folder_name,\n",
    "                      run,\n",
    "                      weights,\n",
    "                      losses_train,\n",
    "                      losses_val,\n",
    "                      uni_dists,\n",
    "                      run_time,\n",
    "                      cl=cl,\n",
    "                      losses_val_all_states = losses_val_all_states\n",
    "                      )\n",
    "\n",
    "        print(\"-------------------------------------------\")\n",
    "        print()\n",
    "    \n",
    "    save_average_plots(time_now, folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset...\n",
      "Dataset generated - 3s\n",
      "\n",
      "Max train / Last run\n",
      "-------------------------------------------\n",
      "  CL   | Run |   Iter    |Acc train| Time  \n",
      "-------------------------------------------\n",
      " NCL   |   0 |  237/1999 |  11/11  | 197\n",
      "-------------------------------------------\n",
      "\n",
      "Generating dataset...\n",
      "Dataset generated - 3s\n",
      "\n",
      "Max train / Last run\n",
      "-------------------------------------------\n",
      "  CL   | Run |   Iter    |Acc train| Time  \n",
      "-------------------------------------------\n",
      " NCL   |   1 |  849/1999 |  11/11  | 97\n",
      "-------------------------------------------\n",
      "\n",
      "Generating dataset...\n",
      "Dataset generated - 3s\n",
      "\n",
      "Max train / Last run\n",
      "-------------------------------------------\n",
      "  CL   | Run |   Iter    |Acc train| Time  \n",
      "-------------------------------------------\n",
      " NCL   |   2 |  946/1999 |  11/11  | 96\n",
      "-------------------------------------------\n",
      "\n",
      "Generating dataset...\n",
      "Dataset generated - 3s\n",
      "\n",
      "Max train / Last run\n",
      "-------------------------------------------\n",
      "  CL   | Run |   Iter    |Acc train| Time  \n",
      "-------------------------------------------\n",
      " NCL   |   3 | 1333/1999 |  10/10  | 97\n",
      "-------------------------------------------\n",
      "\n",
      "Generating dataset...\n",
      "Dataset generated - 3s\n",
      "\n",
      "Max train / Last run\n",
      "-------------------------------------------\n",
      "  CL   | Run |   Iter    |Acc train| Time  \n",
      "-------------------------------------------\n",
      " NCL   |   4 | 1931/1999 |  11/11  | 95\n",
      "-------------------------------------------\n",
      "\n",
      "Generating dataset...\n",
      "Dataset generated - 3s\n",
      "\n",
      "Max train / Last run\n",
      "-------------------------------------------\n",
      "  CL   | Run |   Iter    |Acc train| Time  \n",
      "-------------------------------------------\n",
      " NCL   |   5 | 1792/1999 |  11/11  | 97\n",
      "-------------------------------------------\n",
      "\n",
      "Generating dataset...\n",
      "Dataset generated - 3s\n",
      "\n",
      "Max train / Last run\n",
      "-------------------------------------------\n",
      "  CL   | Run |   Iter    |Acc train| Time  \n",
      "-------------------------------------------\n",
      " NCL   |   6 | 1335/1999 |  11/11  | 96\n",
      "-------------------------------------------\n",
      "\n",
      "Generating dataset...\n",
      "Dataset generated - 3s\n",
      "\n",
      "Max train / Last run\n",
      "-------------------------------------------\n",
      "  CL   | Run |   Iter    |Acc train| Time  \n",
      "-------------------------------------------\n",
      " NCL   |   7 |    2/1999 |  11/11  | 101\n",
      "-------------------------------------------\n",
      "\n",
      "Generating dataset...\n",
      "Dataset generated - 3s\n",
      "\n",
      "Max train / Last run\n",
      "-------------------------------------------\n",
      "  CL   | Run |   Iter    |Acc train| Time  \n",
      "-------------------------------------------\n",
      " NCL   |   8 |  412/1999 |  11/13  | 99\n",
      "-------------------------------------------\n",
      "\n",
      "Generating dataset...\n",
      "Dataset generated - 3s\n",
      "\n",
      "Max train / Last run\n",
      "-------------------------------------------\n",
      "  CL   | Run |   Iter    |Acc train| Time  \n",
      "-------------------------------------------\n",
      " NCL   |   9 |   11/1999 |  11/11  | 98\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for dist_type in [\"fro\", np.inf]:\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matrices():\n",
    "    def print_matrix(matrix):\n",
    "        for row in matrix:\n",
    "            for value in row:\n",
    "                print(f\"{round(value.real, 3) + round(value.imag, 3) * 1j:14}\", end='\\t')\n",
    "            print('\\n')\n",
    "\n",
    "    time_now = \"2024-02-06 18-39-30\"\n",
    "    run = 0\n",
    "\n",
    "    folder_name = f\"Results/{nqubits}q - {num_iters:} iters/\"\n",
    "    data_file_name = f\"{folder_name}/{time_now} - Data - NCL.csv\"\n",
    "    read_data = pd.read_csv(data_file_name,\n",
    "                            usecols=[\"weights\"],\n",
    "                            converters={\"weights\":ast.literal_eval})\n",
    "\n",
    "    all_runs_weights = list(map(np.array, read_data[\"weights\"]))\n",
    "    weights = all_runs_weights[run][num_iters-1]\n",
    "\n",
    "    unitary_matrix = qml.matrix(hamiltonian_unitary)()\n",
    "    variational_matrix = qml.matrix(variational_unitary)(weights)\n",
    "    fro_norm = jnp.linalg.norm(variational_matrix - unitary_matrix, ord=\"fro\")\n",
    "    inf_norm = jnp.linalg.norm(variational_matrix - unitary_matrix, ord=np.inf)\n",
    "\n",
    "    clean_unitary_matrix = np.array([[v if np.abs(v)>10**(-1) else 0 for v in r] for r in unitary_matrix])\n",
    "    clean_variational_matrix = np.array([[v if np.abs(v)>10**(-1) else 0 for v in r] for r in variational_matrix])\n",
    "\n",
    "\n",
    "    data = unitary_matrix - variational_matrix\n",
    "    data = pd.DataFrame(data)\n",
    "    data_file_name = f\"{folder_name}/{time_now} - Matrix difference - run {run}.csv\"\n",
    "    data.to_csv(data_file_name, index=False, header=False)\n",
    "    \n",
    "    data2 = clean_unitary_matrix - clean_variational_matrix\n",
    "    data2 = pd.DataFrame(data2)\n",
    "    data_file_name2 = f\"{folder_name}/{time_now} - Matrix difference clean - run {run}.csv\"\n",
    "    data2.to_csv(data_file_name2, index=False, header=False)\n",
    "\n",
    "    # print_matrix(clean_unitary_matrix)\n",
    "    # print()\n",
    "    # print_matrix(clean_variational_matrix)\n",
    "    # print()\n",
    "    # print(fro_norm)\n",
    "    # print(inf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = \"2024-02-06 18-39-30\"\n",
    "run = 0\n",
    "\n",
    "folder_name = f\"Results/{nqubits}q - {num_iters:} iters/\"\n",
    "data_file_name = f\"{folder_name}/{time_now} - Data - NCL.csv\"\n",
    "read_data = pd.read_csv(data_file_name,\n",
    "                        usecols=[\"weights\"],\n",
    "                        converters={\"weights\":ast.literal_eval})\n",
    "\n",
    "all_runs_weights = list(map(np.array, read_data[\"weights\"]))\n",
    "weights = all_runs_weights[run][num_iters-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = all_runs_weights[run][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitary_matrix = qml.matrix(hamiltonian_unitary)()\n",
    "variational_matrix = qml.matrix(variational_unitary)(weights)\n",
    "diff_matrix = unitary_matrix - variational_matrix\n",
    "\n",
    "fro_norm = jnp.linalg.norm(diff_matrix, ord=\"fro\")\n",
    "inf_norm = jnp.linalg.norm(diff_matrix, ord=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_unitary_matrix = np.array([[v if np.abs(v)>10**(-1) else 0 for v in r] for r in unitary_matrix])\n",
    "clean_variational_matrix = np.array([[v if np.abs(v)>10**(-1) else 0 for v in r] for r in variational_matrix])\n",
    "clean_diff_matrix = clean_unitary_matrix - clean_variational_matrix\n",
    "\n",
    "clean_fro_norm = jnp.linalg.norm(clean_diff_matrix, ord=\"fro\")\n",
    "clean_inf_norm = jnp.linalg.norm(clean_diff_matrix, ord=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.327905790605817 10.898848660369541\n",
      "9.925183900614426 8.598933662401748\n"
     ]
    }
   ],
   "source": [
    "print(fro_norm, clean_fro_norm)\n",
    "print(inf_norm, clean_inf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002566376434289157\n"
     ]
    }
   ],
   "source": [
    "ini_state = np.array([0, 1]+[0]*(2**nqubits-2))\n",
    "target_state = unitary_matrix @ ini_state\n",
    "out_state = variational_matrix @ ini_state\n",
    "\n",
    "loss = jnp.abs(jnp.vdot(out_state, target_state))**2\n",
    "\n",
    "clean_target_state = np.array([v if np.abs(v)>10**(-1) else 0 for v in target_state])\n",
    "clean_out_state = np.array([v if np.abs(v)>10**(-1) else 0 for v in out_state])\n",
    "\n",
    "clean_loss = jnp.abs(jnp.vdot(clean_out_state, clean_target_state))**2\n",
    "\n",
    "\n",
    "print(clean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('clean_target_state.csv', clean_target_state, delimiter=',')\n",
    "np.savetxt('clean_out_state.csv', clean_out_state, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_unitary_matrix = np.array([[np.abs(v) for v in r] for r in unitary_matrix])\n",
    "abs_variational_matrix = np.array([[np.abs(v) for v in r] for r in variational_matrix])\n",
    "abs_diff_matrix = abs_unitary_matrix - abs_variational_matrix\n",
    "\n",
    "abs_fro_norm = jnp.linalg.norm(abs_diff_matrix, ord=\"fro\")\n",
    "abs_inf_norm = jnp.linalg.norm(abs_diff_matrix, ord=np.inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.327905790605817 10.898848660369541 9.181683363272231\n",
      "9.925183900614426 8.598933662401748 8.359812921184403\n"
     ]
    }
   ],
   "source": [
    "print(fro_norm, clean_fro_norm, abs_fro_norm)\n",
    "print(inf_norm, clean_inf_norm, abs_inf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_phase_unitary_dist(U, V):\n",
    "    N = U.shape[0]\n",
    "    return np.sqrt(1 - np.abs(np.trace(U.conj().T @ V))/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9897180782176259\n"
     ]
    }
   ],
   "source": [
    "print(global_phase_unitary_dist(unitary_matrix, variational_matrix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
