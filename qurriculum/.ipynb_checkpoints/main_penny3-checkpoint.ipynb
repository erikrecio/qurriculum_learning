{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "from pennylane.optimize import NesterovMomentumOptimizer\n",
    "from shapely.geometry import Polygon, Point\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "plt.ioff()\n",
    "\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pytz\n",
    "import ast\n",
    "from pypdf import PdfMerger\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Running parameters\n",
    "num_iters = 2000    # Number of training iterations\n",
    "num_runs = 5\n",
    "with_cv = False\n",
    "with_val = False\n",
    "\n",
    "# Circuit and optimization parameters\n",
    "nqubits = 4         # Num qubits, min 4, always 2**num_layers qubits\n",
    "with_bias = False    # Add a bias to the output of the quantum circuit\n",
    "random = True\n",
    "optimizer = \"GradientDescent\"  # \"Adam\", \"Nesterov\", \"GradientDescent\"\n",
    "loss_type = \"cross-entropy\" # \"projectors\", \"cross-entropy\"\n",
    "\n",
    "# Data hyper-parameters\n",
    "batch_size = 100     # batch training size\n",
    "train_size = 100      # Total ground states that will be used for training\n",
    "val_size = 1000      # Total gound states with training + validation\n",
    "cv_ratios = [0.3, 0.2, 0.2, 0.2, 0.1]\n",
    "\n",
    "# How the training data is generated\n",
    "uniform_train = False    # True - Uniform, False - Balanced\n",
    "epsilon_train = True   # True - epsilon, False - no epsilon\n",
    "uniform_val = True\n",
    "epsilon_val = False\n",
    "\n",
    "# Multiprocess hyperparameters. recommended: (num_data, num_cpus) -> (20, 10), (100, 20-30), (200, 40-50) (1000, 50-70)\n",
    "num_cpus_batch = 20\n",
    "num_cpus_train = 20\n",
    "num_cpus_val = 60\n",
    "\n",
    "# Tweak training hyper-parameters\n",
    "max_weight_init = 2*np.pi  # weight_init goes from 0 to this number. Max = 2*np.pi. Other options = 0.01\n",
    "stepsize = 0.01         # stepsize of the gradient descent.\n",
    "\n",
    "# Constant definitions\n",
    "layers = int(np.log2(nqubits))\n",
    "nweights = 30*(layers-1) + 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def X(i):\n",
    "    return qml.PauliX(i)\n",
    "\n",
    "def Z(i):\n",
    "    return qml.PauliZ(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Gound states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_state(j1, j2):\n",
    "    \n",
    "    hamiltonian = 0\n",
    "    for i in range(nqubits):\n",
    "        hamiltonian += Z(i)\n",
    "        hamiltonian -= j1 * X(i) @ X((i+1)%nqubits)\n",
    "        hamiltonian -= j2 * X((i-1)%nqubits) @ Z(i) @ X((i+1)%nqubits)\n",
    "    \n",
    "    _, eigvecs = np.linalg.eigh(qml.matrix(hamiltonian))\n",
    "    \n",
    "    return eigvecs[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define coordinates of the points of each region# Definir las coordenadas de los puntos de cada región\n",
    "region01_coords = np.array([(-2, 1), (2, 1), (4, 3), (4, 4), (-4, 4), (-4, 3)])    # Class 0\n",
    "region02_coords = np.array([(-3, -4), (0, -1), (3, -4)])                           # Class 0\n",
    "region1_coords = np.array([(0, -1), (3, -4), (4, -4), (4, 3)])                     # Class 1\n",
    "region2_coords = np.array([(0, -1), (-3, -4), (-4, -4), (-4, 3)])                  # Class 2\n",
    "region3_coords = np.array([(-2, 1), (2, 1), (0, -1)])                              # Class 3\n",
    "\n",
    "e = 0.2\n",
    "# Define coordinates of the points of each region far from the borders\n",
    "region01e_coords = np.array([(-2+(np.sqrt(2)-1)*e, 1+e), (2-(np.sqrt(2)-1)*e, 1+e), (4, 3+np.sqrt(2)*e), (4, 4), (-4, 4), (-4, 3+np.sqrt(2)*e)])    # Class 0 with epsilon\n",
    "region02e_coords = np.array([(-3+np.sqrt(2)*e, -4), (0, -1-np.sqrt(2)*e), (3-np.sqrt(2)*e, -4)])                                                    # Class 0 with epsilon\n",
    "region1e_coords = np.array([(0+np.sqrt(2)*e, -1), (3+np.sqrt(2)*e, -4), (4, -4), (4, 3-np.sqrt(2)*e)])                                              # Class 1 with epsilon\n",
    "region2e_coords = np.array([(0-np.sqrt(2)*e, -1), (-3-np.sqrt(2)*e, -4), (-4, -4), (-4, 3-np.sqrt(2)*e)])                                           # Class 2 with epsilon\n",
    "region3e_coords = np.array([(-2+e/np.tan(np.pi/8), 1-e), (2-e/np.tan(np.pi/8), 1-e), (0, -1+np.sqrt(2)*e)])                                         # Class 3 with epsilon\n",
    "\n",
    "\n",
    "def labeling(x, y):\n",
    "\n",
    "    # Crear objetos Polygon para cada región\n",
    "    region01_poly = Polygon(region01_coords)\n",
    "    region02_poly = Polygon(region02_coords)\n",
    "    region1_poly = Polygon(region1_coords)\n",
    "    region2_poly = Polygon(region2_coords)\n",
    "    region3_poly = Polygon(region3_coords)\n",
    "    \n",
    "    \n",
    "    p = Point(x, y)\n",
    "    if region01_poly.contains(p):\n",
    "        return 0\n",
    "    elif region02_poly.contains(p):\n",
    "        return 0\n",
    "    elif region1_poly.contains(p):\n",
    "        return 1\n",
    "    elif region2_poly.contains(p):\n",
    "        return 2\n",
    "    elif region3_poly.contains(p):\n",
    "        return 3\n",
    "    else:\n",
    "        return None # if the point is not in any region\n",
    "    \n",
    "\n",
    "def labeling_epsilon(x, y):\n",
    "    \n",
    "    # Crear objetos Polygon para cada región\n",
    "    region01e_poly = Polygon(region01e_coords)\n",
    "    region02e_poly = Polygon(region02e_coords)\n",
    "    region1e_poly = Polygon(region1e_coords)\n",
    "    region2e_poly = Polygon(region2e_coords)\n",
    "    region3e_poly = Polygon(region3e_coords)\n",
    "    \n",
    "    \n",
    "    p = Point(x, y)\n",
    "    if region01e_poly.contains(p):\n",
    "        return 0\n",
    "    elif region02e_poly.contains(p):\n",
    "        return 0\n",
    "    elif region1e_poly.contains(p):\n",
    "        return 1\n",
    "    elif region2e_poly.contains(p):\n",
    "        return 2\n",
    "    elif region3e_poly.contains(p):\n",
    "        return 3\n",
    "    else:\n",
    "        return None # if the point is not in any region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ground states\n",
    "def generate_gs(num_points, uniform, epsilon, num_cpus):\n",
    "\n",
    "    if random:\n",
    "        rng = np.random\n",
    "    else:\n",
    "        rng = np.random.RandomState(0)\n",
    "        \n",
    "\n",
    "    if uniform:\n",
    "        if epsilon:\n",
    "            j_list = []\n",
    "            num = 0\n",
    "            while num < num_points:\n",
    "                j = rng.uniform(-4, 4, 2)\n",
    "                l = labeling_epsilon(j[0], j[1])\n",
    "\n",
    "                if l in [0,1,2,3]:\n",
    "                    num += 1\n",
    "                    j_list.append(j)\n",
    "\n",
    "            j_list = np.array(j_list)\n",
    "            \n",
    "        else:\n",
    "            j_list = rng.uniform(-4, 4, (num_points,2))\n",
    "    \n",
    "    \n",
    "    else: # Then it's balanced\n",
    "        npoints_class = num_points//4\n",
    "        num_points = 4*npoints_class\n",
    "        \n",
    "        npoints_02 = npoints_class//2\n",
    "        npoints_01 = npoints_class - npoints_02\n",
    "        \n",
    "        j_list = []\n",
    "        num_0, num_1, num_2, num_3 = 0, 0, 0, 0\n",
    "        num_01, num_02 = 0, 0\n",
    "        \n",
    "        while num_0 != npoints_class or num_1 != npoints_class or num_2 != npoints_class or num_3 != npoints_class:\n",
    "            j = rng.uniform(-4, 4, 2)\n",
    "            l = labeling_epsilon(j[0], j[1]) if epsilon else labeling(j[0], j[1])\n",
    "\n",
    "            if l==0 and num_0 < npoints_class:\n",
    "                \n",
    "                p = Point(j[0], j[1])\n",
    "                if Polygon(region01_coords).contains(p) and num_01 < npoints_01:\n",
    "                    num_01 += 1\n",
    "                    num_0 += 1\n",
    "                    j_list.append(j)\n",
    "                elif Polygon(region02_coords).contains(p) and num_02 < npoints_02:\n",
    "                    num_02 += 1\n",
    "                    num_0 += 1\n",
    "                    j_list.append(j)\n",
    "                \n",
    "            elif l==1 and num_1 < npoints_class:\n",
    "                num_1 += 1\n",
    "                j_list.append(j)\n",
    "            elif l==2 and num_2 < npoints_class:\n",
    "                num_2 += 1\n",
    "                j_list.append(j)\n",
    "            elif l==3 and num_3 < npoints_class:\n",
    "                num_3 += 1\n",
    "                j_list.append(j)\n",
    "\n",
    "        j_list = np.array(j_list)\n",
    "    \n",
    "    \n",
    "    if num_cpus == 0:\n",
    "        gs_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        for i in range(num_points):\n",
    "            gs_list.append(ground_state(j_list[i,0], j_list[i,1]))\n",
    "            labels_list.append(labeling(j_list[i,0], j_list[i,1]))\n",
    "            \n",
    "    else:\n",
    "        args = [[j_list[i,0], j_list[i,1]] for i in range(num_points)]\n",
    "        \n",
    "        with mp.Pool(num_cpus) as pool:\n",
    "            gs_list = pool.starmap(ground_state, args)\n",
    "        \n",
    "        with mp.Pool(num_cpus) as pool:\n",
    "            labels_list = pool.starmap(labeling, args)\n",
    "\n",
    "\n",
    "    gs_list = np.array(gs_list)\n",
    "    labels_list = np.array(labels_list)\n",
    "\n",
    "    return gs_list, labels_list, j_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convolutional_layer(q1, q2, weights):\n",
    "    qml.U3(wires=q1, theta=weights[0], phi=weights[1], delta=weights[2])\n",
    "    qml.U3(wires=q1, theta=weights[3], phi=weights[4], delta=weights[5])\n",
    "    qml.CNOT(wires=[q2, q1])\n",
    "    qml.RZ(wires=q1, phi=weights[6])\n",
    "    qml.RY(wires=q2, phi=weights[7])\n",
    "    qml.CNOT(wires=[q1, q2])\n",
    "    qml.RY(wires=q2, phi=weights[8])\n",
    "    qml.CNOT(wires=[q2, q1])\n",
    "    qml.U3(wires=q1, theta=weights[9], phi=weights[10], delta=weights[11])\n",
    "    qml.U3(wires=q1, theta=weights[12], phi=weights[13], delta=weights[14])\n",
    "\n",
    "def pooling_layer(q1, q2, weights):\n",
    "    qml.U3(wires=q1, theta=weights[0], phi=weights[1], delta=weights[2])\n",
    "    qml.U3(wires=q1, theta=weights[3], phi=weights[4], delta=weights[5])\n",
    "    qml.CNOT(wires=[q2, q1])\n",
    "    qml.RZ(wires=q1, phi=weights[6])\n",
    "    qml.RY(wires=q2, phi=weights[7])\n",
    "    qml.CNOT(wires=[q1, q2])\n",
    "    qml.RY(wires=q2, phi=weights[8])\n",
    "    qml.CNOT(wires=[q2, q1])\n",
    "    qml.U3(wires=q1, theta=weights[9], phi=weights[10], delta=weights[11])\n",
    "    qml.U3(wires=q1, theta=weights[12], phi=weights[13], delta=weights[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_circuit(weights, state_ini):\n",
    "    \n",
    "    qubits = list(range(nqubits))\n",
    "    \n",
    "    qml.QubitStateVector(state_ini, wires=qubits)\n",
    "\n",
    "    for j in range(layers-1):\n",
    "        \n",
    "        len_qubits = len(qubits)\n",
    "        \n",
    "        for i in range(len_qubits//2):\n",
    "            convolutional_layer(qubits[2*i], qubits[(2*i+1)%len_qubits], weights[15*2*j:15*(2*j+1)])\n",
    "        \n",
    "        for i in range(len_qubits//2):\n",
    "            convolutional_layer(qubits[2*i+1], qubits[(2*i+2)%len_qubits], weights[15*2*j:15*(2*j+1)])\n",
    "            \n",
    "        for i in range(len_qubits//2):\n",
    "            pooling_layer(qubits[2*i], qubits[(2*i+1)%len_qubits], weights[15*(2*j+1):15*(2*j+2)])\n",
    "\n",
    "        qub = []\n",
    "        for i in range(len_qubits):\n",
    "            if i%2 == 1:\n",
    "                qub.append(qubits[i])\n",
    "                \n",
    "        qubits = qub\n",
    "    \n",
    "    convolutional_layer(qubits[0], qubits[1], weights[15*(2*layers-2):15*(2*layers-1)])\n",
    "    \n",
    "    return qml.expval(Z(qubits[0])), qml.expval(Z(qubits[1])), qml.expval(Z(qubits[0]) @ Z(qubits[1]))\n",
    "\n",
    "# dev_draw = qml.device(\"qiskit.aer\", wires=nqubits)\n",
    "dev = qml.device(\"default.qubit\", wires=nqubits)\n",
    "\n",
    "# cnn_draw = qml.QNode(cnn_circuit, dev_draw)\n",
    "cnn_circuit = qml.QNode(cnn_circuit, dev, interface=\"auto\", diff_method=\"best\")\n",
    "\n",
    "\n",
    "def cnn(weights, state_ini):\n",
    "    z0, z1, zz01 = cnn_circuit(weights, state_ini)\n",
    "\n",
    "    proj_00 = (1+zz01+z0+z1)/4\n",
    "    proj_01 = (1-zz01-z0+z1)/4\n",
    "    proj_10 = (1-zz01+z0-z1)/4\n",
    "    proj_11 = (1+zz01-z0-z1)/4\n",
    "\n",
    "    return pnp.array([proj_00, proj_01, proj_10, proj_11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def variational_classifier(weights, bias, state_ini):\n",
    "    return cnn(weights, state_ini) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# draw circuit\n",
    "\n",
    "# weights = np.random.uniform(0, 2*np.pi, nweights)\n",
    "# drawer = qml.draw(cnn_circuit)\n",
    "# print(drawer(weights, gs_list[0]))\n",
    "\n",
    "# z0, z1, zz01 = cnn_draw(gs_list[0], weights)\n",
    "# print(z0, z1, zz01)\n",
    "# dev_draw._circuit.draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log2_arraybox(abox):\n",
    "    return pnp._np.numpy_boxes.ArrayBox(value = np.log2(abox._value), trace = abox._trace, node = abox._node)\n",
    "\n",
    "\n",
    "def single_loss(weights, bias, ground_state, label):\n",
    "    \n",
    "    proj = variational_classifier(weights, bias, ground_state)\n",
    "\n",
    "    if loss_type == \"projectors\":\n",
    "            cost = proj[label]\n",
    "\n",
    "    elif loss_type == \"cross-entropy\":\n",
    "        \n",
    "            if isinstance(proj[label], pnp._np.numpy_boxes.ArrayBox):\n",
    "                cost = -log2_arraybox(proj[label])\n",
    "            else:\n",
    "                cost = -np.log2(proj[label])\n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "def loss(weights, bias, ground_states, labels, num_cpus):\n",
    "    \n",
    "    if num_cpus == 0:\n",
    "        cost = 0\n",
    "\n",
    "        for j in range(len(labels)):\n",
    "            cost += single_loss(weights, bias, ground_states[j], labels[j])\n",
    "    \n",
    "    else:\n",
    "        args = [[weights, bias, ground_states[j], labels[j]] for j in range(len(labels))]\n",
    "\n",
    "        with mp.Pool(num_cpus) as pool:\n",
    "            cost_arr = pool.starmap(single_loss, args)\n",
    "\n",
    "        cost = sum(cost_arr)\n",
    "    \n",
    "    return cost/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_pred(weights, bias, ground_state):\n",
    "    \n",
    "    projectors = variational_classifier(weights, bias, ground_state)\n",
    "    \n",
    "    if loss_type == \"projectors\":\n",
    "        pred = np.argmin(projectors)\n",
    "    elif loss_type == \"cross-entropy\":\n",
    "        pred = np.argmax(projectors)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def pred(weights, bias, ground_states, num_cpus):\n",
    "    \n",
    "    if num_cpus == 0:\n",
    "        predictions = []\n",
    "        \n",
    "        for j in range(len(ground_states)):\n",
    "            predictions.append(single_pred(weights, bias, ground_states[j]))\n",
    "    else:\n",
    "        args = [[weights, bias, ground_states[j]] for j in range(len(ground_states))]\n",
    "\n",
    "        with mp.Pool(num_cpus) as pool:\n",
    "            predictions = pool.starmap(single_pred, args)\n",
    "            \n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "def acc(predictions, labels):\n",
    "    return sum(predictions==labels)*100/len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_multi_image(filename):\n",
    "    pp = PdfPages(filename)\n",
    "    fig_nums = plt.get_fignums()\n",
    "    figs = [plt.figure(n) for n in fig_nums]\n",
    "    for fig in figs:\n",
    "        fig.savefig(pp, format='pdf')\n",
    "    pp.close()\n",
    "\n",
    "def close_all_figures():\n",
    "    fig_nums = plt.get_fignums()\n",
    "    for n in fig_nums:\n",
    "        plt.figure(n)\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "def save_plots(time_now,\n",
    "               folder_name,\n",
    "               file_name,\n",
    "               it_max,\n",
    "               acc_train,\n",
    "               acc_val,\n",
    "               losses,\n",
    "               pred_train,\n",
    "               pred_val,\n",
    "               j_train,\n",
    "               j_val\n",
    "              ):\n",
    "\n",
    "\n",
    "    fig, axis = plt.subplots(1,3)\n",
    "    fig.set_figheight(6.5)\n",
    "    fig.set_figwidth(20)\n",
    "    fig.tight_layout(pad=2, w_pad=3.5)\n",
    "\n",
    "    # ---------------------------------------------------------------------- #\n",
    "    # -------------------- Loss and accuracy figure ------------------------ #\n",
    "    # ---------------------------------------------------------------------- #\n",
    "\n",
    "    iterations = range(1, num_iters+1)\n",
    "\n",
    "    color1 = 'darkred'\n",
    "    axis[0].set_xlabel('Iterations')\n",
    "    axis[0].set_ylabel('Accuracy %', color=color1)\n",
    "    axis[0].plot(iterations, acc_train, label=\"Training\", color=color1)\n",
    "    axis[0].plot(iterations, acc_val, '-.', label=\"Validation\", color=color1)\n",
    "    axis[0].tick_params(axis='y', labelcolor=color1)\n",
    "    axis[0].set_ylim(0,100)\n",
    "\n",
    "    ax2 = axis[0].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color2 = 'darkblue'\n",
    "    ax2.set_ylabel('Loss', color=color2)  # we already handled the x-label with axis[0]\n",
    "    ax2.plot(iterations, losses, label=\"Loss\", color=color2)\n",
    "    ax2.tick_params(axis='y', labelcolor=color2)\n",
    "    # ax2.set_ylim(bottom=0)\n",
    "\n",
    "    # fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    # plt.legend()\n",
    "    axis[0].set_title(f\"Accuracy and Loss - Run {plot_run}\")\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------- #\n",
    "    # ---------------------------- Max iter -------------------------------- #\n",
    "    # ---------------------------------------------------------------------- #\n",
    "\n",
    "    plot_iter = it_max\n",
    "\n",
    "    # define regions coordinates\n",
    "    x01, y01 = region01_coords[:,0], region01_coords[:,1]\n",
    "    x02, y02 = region02_coords[:,0], region02_coords[:,1]\n",
    "    x1, y1 = region1_coords[:,0], region1_coords[:,1]\n",
    "    x2, y2 = region2_coords[:,0], region2_coords[:,1]\n",
    "    x3, y3 = region3_coords[:,0], region3_coords[:,1]\n",
    "\n",
    "    # put the regions into the plot\n",
    "    axis[1].fill(x01, y01, facecolor='lightskyblue')    # class 0\n",
    "    axis[1].fill(x02, y02, facecolor='lightskyblue')    # class 0\n",
    "    axis[1].fill(x1, y1, facecolor='sandybrown')        # class 1\n",
    "    axis[1].fill(x2, y2, facecolor='salmon')            # class 2\n",
    "    axis[1].fill(x3, y3, facecolor='lightgreen')        # class 3\n",
    "\n",
    "    pred_train_plot = np.array(pred_train[plot_iter])\n",
    "    pred_val_plot = np.array(pred_val[plot_iter])\n",
    "\n",
    "    colors = [\"b\", \"orange\", \"r\", \"g\"]\n",
    "\n",
    "    # plot datapoints\n",
    "    for i in range(4):\n",
    "        axis[1].scatter(\n",
    "            j_train[:, 0][pred_train_plot==i],\n",
    "            j_train[:, 1][pred_train_plot==i],\n",
    "            c=colors[i],\n",
    "            marker=\"o\",\n",
    "            edgecolors=\"k\",\n",
    "            label=f\"class {i+1} train\",\n",
    "        )\n",
    "        if with_val:\n",
    "            axis[1].scatter(\n",
    "                j_val[:, 0][pred_val_plot==i],\n",
    "                j_val[:, 1][pred_val_plot==i],\n",
    "                c=colors[i],\n",
    "                marker=\"^\",\n",
    "                edgecolors=\"k\",\n",
    "                label=f\"class {i+1} validation\",\n",
    "            )\n",
    "\n",
    "\n",
    "    # plt.legend()\n",
    "    axis[1].set_title(f\"Max iteration ({total_acc_train[plot_run][plot_iter]:.0f}%/{total_acc_val[plot_run][plot_iter]:.0f}%)\")\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------- #\n",
    "    # ---------------------------- Last iter ------------------------------- #\n",
    "    # ---------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "    plot_iter = num_iters-1\n",
    "\n",
    "    # define regions coordinates\n",
    "    x01, y01 = region01_coords[:,0], region01_coords[:,1]\n",
    "    x02, y02 = region02_coords[:,0], region02_coords[:,1]\n",
    "    x1, y1 = region1_coords[:,0], region1_coords[:,1]\n",
    "    x2, y2 = region2_coords[:,0], region2_coords[:,1]\n",
    "    x3, y3 = region3_coords[:,0], region3_coords[:,1]\n",
    "\n",
    "    # put the regions into the plot\n",
    "    axis[2].fill(x01, y01, facecolor='lightskyblue')    # class 0\n",
    "    axis[2].fill(x02, y02, facecolor='lightskyblue')    # class 0\n",
    "    axis[2].fill(x1, y1, facecolor='sandybrown')        # class 1\n",
    "    axis[2].fill(x2, y2, facecolor='salmon')            # class 2\n",
    "    axis[2].fill(x3, y3, facecolor='lightgreen')        # class 3\n",
    "\n",
    "    pred_train_plot = np.array(pred_train[plot_iter])\n",
    "    pred_val_plot = np.array(pred_val[plot_iter])\n",
    "\n",
    "    colors = [\"b\", \"orange\", \"r\", \"g\"]\n",
    "\n",
    "    # plot datapoints\n",
    "    for i in range(4):\n",
    "        axis[2].scatter(\n",
    "            j_train[:, 0][pred_train_plot==i],\n",
    "            j_train[:, 1][pred_train_plot==i],\n",
    "            c=colors[i],\n",
    "            marker=\"o\",\n",
    "            edgecolors=\"k\",\n",
    "            label=f\"class {i+1} train\",\n",
    "        )\n",
    "        if with_val:\n",
    "            axis[2].scatter(\n",
    "                j_val[:, 0][pred_val_plot==i],\n",
    "                j_val[:, 1][pred_val_plot==i],\n",
    "                c=colors[i],\n",
    "                marker=\"^\",\n",
    "                edgecolors=\"k\",\n",
    "                label=f\"class {i+1} validation\",\n",
    "            )\n",
    "\n",
    "\n",
    "    # plt.legend()\n",
    "    axis[2].set_title(f\"Last iteration ({total_acc_train[plot_run][plot_iter]:.0f}%/{total_acc_val[plot_run][plot_iter]:.0f}%)\")\n",
    "\n",
    "    # ---------------------------------------------------------------------- #\n",
    "    # --------------------------- Save plots ------------------------------- #\n",
    "    # ---------------------------------------------------------------------- #\n",
    "\n",
    "    plots_pdf_name = f\"{folder_name}/{time_now} - Plots - {file_name}.pdf\"\n",
    "    save_multi_image(plots_pdf_name + \"2\")\n",
    "    close_all_figures()\n",
    "    \n",
    "    # Merge the new plot with the rest and delete the last file\n",
    "    merger = PdfMerger()\n",
    "    merger.append(plots_pdf_name)\n",
    "    merger.append(plots_pdf_name + \"2\")\n",
    "    merger.write(plots_pdf_name)\n",
    "    merger.close()\n",
    "    \n",
    "    os.remove(plots_pdf_name + \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hyperparameters(time_now, folder_name, file_name):\n",
    "    \n",
    "    # --------------- Hyperparameters -----------------#\n",
    "    hyperparameters = {}\n",
    "    hyperparameters[\"num_iters\"] = [num_iters]\n",
    "    hyperparameters[\"num_runs\"] = [num_runs]\n",
    "    hyperparameters[\"nqubits\"] = [nqubits]\n",
    "    hyperparameters[\"with_bias\"] = [with_bias]\n",
    "    hyperparameters[\"random\"] = [random]\n",
    "    hyperparameters[\"optimizer\"] = [optimizer]\n",
    "    hyperparameters[\"loss_type\"] = [loss_type]\n",
    "    hyperparameters[\"batch_size\"] = [batch_size]\n",
    "    hyperparameters[\"train_size\"] = [train_size]\n",
    "    hyperparameters[\"val_size\"] = [val_size]\n",
    "    hyperparameters[\"uniform_train\"] = [uniform_train]\n",
    "    hyperparameters[\"epsilon_train\"] = [epsilon_train]\n",
    "    hyperparameters[\"uniform_val\"] = [uniform_val]\n",
    "    hyperparameters[\"epsilon_val\"] = [epsilon_val]\n",
    "    hyperparameters[\"num_cpus_batch\"] = [num_cpus_batch]\n",
    "    hyperparameters[\"num_cpus_train\"] = [num_cpus_train]\n",
    "    hyperparameters[\"num_cpus_val\"] = [num_cpus_val]\n",
    "    hyperparameters[\"max_weight_init\"] = [max_weight_init]\n",
    "    hyperparameters[\"stepsize\"] = [stepsize]\n",
    "\n",
    "    hyperparameters = pd.DataFrame(hyperparameters)\n",
    "\n",
    "    hyperparameters_file_name = f\"{folder_name}/{time_now} - Hyperparameters - {file_name}.csv\"\n",
    "    hyperparameters.to_csv(hyperparameters_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_2(time_now,\n",
    "                folder_name,\n",
    "                run,\n",
    "                weights,\n",
    "                bias,\n",
    "                losses,\n",
    "                j_train,\n",
    "                j_val,\n",
    "                pred_train,\n",
    "                pred_val,\n",
    "                acc_train,\n",
    "                acc_val,\n",
    "                run_time,\n",
    "                cv\n",
    "               ):\n",
    "    \n",
    "    if cv:\n",
    "        file_name = f\"CV\"# - {train_size} Train - {val_size} Val\"\n",
    "    else:\n",
    "        file_name = f\"NCV\"# - {train_size} Train - {val_size} Val\"\n",
    "        \n",
    "    # -------------------- Total Data -------------------- #\n",
    "    data = {}\n",
    "    data[\"run\"] = run\n",
    "    \n",
    "    it_max = np.argmax(np.array(acc_train))\n",
    "    acc_train_max = acc_train[it_max]\n",
    "    acc_val_max = acc_val[it_max]\n",
    "    \n",
    "    data[\"it_max\"] = it_max\n",
    "    data[\"acc_train_max\"] = acc_train_max\n",
    "    data[\"acc_train_last\"] = acc_train_last\n",
    "    data[\"acc_val_max\"] = acc_val_max\n",
    "    data[\"acc_val_last\"] = acc_val_last\n",
    "    data[\"run_time\"] = run_time\n",
    "    \n",
    "    data[\"weights\"] = weights\n",
    "    data[\"bias\"] = bias\n",
    "    data[\"losses\"] = losses\n",
    "    data[\"j_train\"] = j_train\n",
    "    data[\"j_val\"] = j_val\n",
    "    data[\"pred_train\"] = pred_train\n",
    "    data[\"pred_val\"] = pred_val\n",
    "    data[\"acc_train\"] = acc_train\n",
    "    data[\"acc_val\"] = acc_val\n",
    "    \n",
    "\n",
    "    data = pd.DataFrame(data)\n",
    "    \n",
    "    data_file_name = f\"{folder_name}/{time_now} - Data - {file_name}.csv\"\n",
    "    data.to_csv(data_file_name, mode='a', index=False, header=not os.path.exists(data_file_name))\n",
    "    \n",
    "    \n",
    "    # ------------------- Results ------------------- #\n",
    "    \n",
    "    read_data = pd.read_csv(data_file_name,\n",
    "                     usecols=[\"it_max\",\n",
    "                              \"acc_train_max\",\n",
    "                              \"acc_val_max\",\n",
    "                              \"acc_train\",\n",
    "                              \"acc_val\"],\n",
    "                     converters={\"acc_train\":ast.literal_eval,\n",
    "                                 \"acc_val\":ast.literal_eval})\n",
    "    \n",
    "    total_it_max = read_data[\"it_max\"]\n",
    "    total_acc_train_max = read_data[\"acc_train_max\"]\n",
    "    total_acc_val_max = read_data[\"acc_val_max\"]\n",
    "    total_acc_train = read_data[\"acc_train\"].tolist()\n",
    "    total_acc_val = read_data[\"acc_val\"].tolist()\n",
    "    \n",
    "    best_run_max = total_acc_train_max.argmax()\n",
    "    best_it_max = total_it_max[run_max]\n",
    "    avg_acc_train_max = total_acc_train_max.mean()\n",
    "    avg_acc_val_max = total_acc_train_max.mean()\n",
    "    \n",
    "    best_it_last = num_iters-1\n",
    "    best_run_last = np.argmax(np.array(total_acc_train)[:,best_it_last])\n",
    "    avg_acc_train_last = np.mean(np.array(total_acc_train)[:,best_it_last])\n",
    "    avg_acc_val_last = np.mean(np.array(total_acc_val)[:,best_it_last])\n",
    "\n",
    "    \n",
    "    results = {}\n",
    "    results[\"num_runs\"] = [run+1]\n",
    "    results[\"best_run_max\"] = [best_run_max]\n",
    "    results[\"best_run_last\"] = [best_run_last]\n",
    "    results[\"best_it_max\"] = [best_it_max]\n",
    "    results[\"best_it_last\"] = [best_it_last]\n",
    "    results[\"best_acc_train_max\"] = [total_acc_train[run_max][it_max_run_max]]\n",
    "    results[\"best_acc_train_last\"] = [total_acc_train[run_max_last][num_iters-1]]\n",
    "    results[\"best_acc_val_max\"] = [total_acc_val[run_max][it_max_run_max]]\n",
    "    results[\"best_acc_val_last\"] = [total_acc_val[run_max_last][num_iters-1]]\n",
    "    results[\"avg_acc_train_max\"] = [avg_acc_train_max]\n",
    "    results[\"avg_acc_train_last\"] = [avg_acc_train_last]\n",
    "    results[\"avg_acc_val_max\"] = [avg_acc_val_max]\n",
    "    results[\"avg_acc_val_last\"] = [avg_acc_val_last]\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    results_file_name = f\"{folder_name}/{time_now} - Results - {file_name}.csv\"\n",
    "    results.to_csv(results_file_name, index=False)\n",
    "    \n",
    "    \n",
    "    # ------------------- Plots ------------------- #\n",
    "    save_plots(time_now,\n",
    "               folder_name,\n",
    "               file_name,\n",
    "               it_max,\n",
    "               acc_train,\n",
    "               acc_val,\n",
    "               losses,\n",
    "               pred_train,\n",
    "               pred_val,\n",
    "               j_train,\n",
    "               j_val\n",
    "              )\n",
    "    \n",
    "    \n",
    "    print(\n",
    "        f\" {cv} |\"\n",
    "        f\" {run:3d} |\"\n",
    "        f\" {it_max:4d}/{num_iters-1:4d} |\"\n",
    "        f\"  {acc_train[it_max]:0.0f}/{acc_train[num_iters-1]:0.0f}  |\"\n",
    "        f\" {acc_val[it_max]:0.0f}/{acc_val[num_iters-1]:0.0f} |\"\n",
    "        f\" {run_time:0.0f}\"\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_data(time_now,\n",
    "              total_weights,\n",
    "              total_bias,\n",
    "              total_losses,\n",
    "              total_pred_train,\n",
    "              total_pred_val,\n",
    "              total_acc_train,\n",
    "              total_acc_val,\n",
    "              total_it_max,\n",
    "              total_acc_train_max,\n",
    "              total_acc_val_max,\n",
    "              total_j_train,\n",
    "              total_j_val,\n",
    "              total_time,\n",
    "              run_max_last,\n",
    "              avg_acc_train_last,\n",
    "              avg_acc_val_last,\n",
    "              run_max,\n",
    "              it_max_run_max,\n",
    "              avg_acc_train_max,\n",
    "              avg_acc_val_max,\n",
    "              cv\n",
    "             ):\n",
    "    \n",
    "    folder_name = f\"Results/{nqubits}q - {num_iters:} iters/\"\n",
    "        \n",
    "        \n",
    "    if not os.path.isdir(f'{folder_name}'):\n",
    "        os.makedirs(f'{folder_name}')\n",
    "    \n",
    "    # Specifications of the hyperparameteres for the name of the file\n",
    "    # ut = \"unif-T\" if uniform_train else \"no-unif-T\"\n",
    "    # et = \"eps-T\" if epsilon_train else \"no-eps-T\"\n",
    "    # uv = \"unif-V\" if uniform_val else \"no-unif-V\"\n",
    "    # ev = \"eps-V\" if epsilon_val else \"no-eps-V\"\n",
    "    \n",
    "    \n",
    "    if cv:\n",
    "        word_cv = \"CV - \"\n",
    "    else:\n",
    "        word_cv = \"NCV - \"\n",
    "        \n",
    "    \n",
    "    file_name = f\"{word_cv}{train_size} Train - {val_size} Val\"\n",
    "    \n",
    "    \n",
    "    # -------------------- Total Data -------------------- #\n",
    "    data = {}\n",
    "    data[\"total_weights\"] = total_weights\n",
    "    data[\"total_bias\"] = total_bias\n",
    "    data[\"total_losses\"] = total_losses\n",
    "    data[\"total_pred_train\"] = total_pred_train\n",
    "    data[\"total_pred_val\"] = total_pred_val\n",
    "    data[\"total_acc_train\"] = total_acc_train\n",
    "    data[\"total_acc_val\"] = total_acc_val\n",
    "    data[\"total_it_max\"] = total_it_max\n",
    "    data[\"total_acc_train_max\"] = total_acc_train_max\n",
    "    data[\"total_acc_val_max\"] = total_acc_val_max\n",
    "    data[\"total_j_train\"] = total_j_train\n",
    "    data[\"total_j_val\"] = total_j_val\n",
    "    data[\"total_time\"] = total_time\n",
    "\n",
    "    data = pd.DataFrame(data)\n",
    "    \n",
    "    data_file_name = f\"{folder_name}/{time_now} - Data - {file_name}.csv\"\n",
    "    data.to_csv(data_file_name, index=False)\n",
    "    \n",
    "    # --------------- Hyperparameters -----------------#\n",
    "    hyperparameters = {}\n",
    "    hyperparameters[\"num_iters\"] = [num_iters]\n",
    "    hyperparameters[\"num_runs\"] = [num_runs]\n",
    "    hyperparameters[\"nqubits\"] = [nqubits]\n",
    "    hyperparameters[\"with_bias\"] = [with_bias]\n",
    "    hyperparameters[\"random\"] = [random]\n",
    "    hyperparameters[\"optimizer\"] = [optimizer]\n",
    "    hyperparameters[\"loss_type\"] = [loss_type]\n",
    "    hyperparameters[\"batch_size\"] = [batch_size]\n",
    "    hyperparameters[\"train_size\"] = [train_size]\n",
    "    hyperparameters[\"val_size\"] = [val_size]\n",
    "    hyperparameters[\"uniform_train\"] = [uniform_train]\n",
    "    hyperparameters[\"epsilon_train\"] = [epsilon_train]\n",
    "    hyperparameters[\"uniform_val\"] = [uniform_val]\n",
    "    hyperparameters[\"epsilon_val\"] = [epsilon_val]\n",
    "    hyperparameters[\"num_cpus_batch\"] = [num_cpus_batch]\n",
    "    hyperparameters[\"num_cpus_train\"] = [num_cpus_train]\n",
    "    hyperparameters[\"num_cpus_val\"] = [num_cpus_val]\n",
    "    hyperparameters[\"max_weight_init\"] = [max_weight_init]\n",
    "    hyperparameters[\"stepsize\"] = [stepsize]\n",
    "\n",
    "    hyperparameters = pd.DataFrame(hyperparameters)\n",
    "\n",
    "    hyperparameters_file_name = f\"{folder_name}/{time_now} - Hyperparameters - {file_name}.csv\"\n",
    "    hyperparameters.to_csv(hyperparameters_file_name, index=False)\n",
    "    \n",
    "    # ------------------- Results ------------------- #\n",
    "    results = {}\n",
    "    results[\"best_run_max\"] = [run_max]\n",
    "    results[\"best_run_last\"] = [run_max_last]\n",
    "    results[\"best_acc_train_max\"] = [total_acc_train[run_max][it_max_run_max]]\n",
    "    results[\"best_acc_train_last\"] = [total_acc_train[run_max_last][num_iters-1]]\n",
    "    results[\"best_acc_val_max\"] = [total_acc_val[run_max][it_max_run_max]]\n",
    "    results[\"best_acc_val_last\"] = [total_acc_val[run_max_last][num_iters-1]]\n",
    "    results[\"avg_acc_train_max\"] = [avg_acc_train_max]\n",
    "    results[\"avg_acc_train_last\"] = [avg_acc_train_last]\n",
    "    results[\"avg_acc_val_max\"] = [avg_acc_val_max]\n",
    "    results[\"avg_acc_val_last\"] = [avg_acc_val_last]\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    results_file_name = f\"{folder_name}/{time_now} - Results - {file_name}.csv\"\n",
    "    results.to_csv(results_file_name, index=False)\n",
    "    \n",
    "    \n",
    "    # ------------------- Plots ------------------- #\n",
    "    plots_pdf_name = save_plots(time_now,\n",
    "                                folder_name,\n",
    "                                file_name,\n",
    "                                total_it_max,\n",
    "                                total_acc_train,\n",
    "                                total_acc_val,\n",
    "                                total_losses,\n",
    "                                total_pred_train,\n",
    "                                total_pred_val,\n",
    "                                total_j_train,\n",
    "                                total_j_val\n",
    "                               )\n",
    "    \n",
    "    \n",
    "    #return data_file_name, hyperparameters_file_name, results_file_name, plots_pdf_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_qcnn(gs_train, gs_val, labels_train, labels_val, cv):\n",
    "        \n",
    "    if random:\n",
    "        rng = np.random\n",
    "    else:\n",
    "        rng = np.random.RandomState(0)\n",
    "        \n",
    "    # weights and bias initialization\n",
    "    weights_init = pnp.random.uniform(0, max_weight_init, nweights, requires_grad=True)\n",
    "\n",
    "    if with_bias:\n",
    "        bias_init = pnp.array([0.0]*4, requires_grad=True)\n",
    "    else:\n",
    "        bias_init = np.array([0.0]*4)\n",
    "\n",
    "    # choose variational classifier\n",
    "    if optimizer == \"Nesterov\":\n",
    "        opt = NesterovMomentumOptimizer(stepsize)\n",
    "    elif optimizer == \"Adam\":\n",
    "        opt = qml.AdamOptimizer(stepsize=stepsize, beta1=0.9, beta2=0.99, eps=1e-08)\n",
    "    elif optimizer == \"GradientDescent\":\n",
    "        opt = qml.GradientDescentOptimizer(stepsize)\n",
    "\n",
    "    #Initiaize variables\n",
    "    weights = []\n",
    "    bias = []\n",
    "    losses = []\n",
    "    pred_train_arr = []\n",
    "    pred_val_arr = []\n",
    "    acc_train_arr = []\n",
    "    acc_val_arr = []\n",
    "\n",
    "    w = weights_init\n",
    "    b = bias_init\n",
    "    \n",
    "    for it in range(num_iters):\n",
    "        \n",
    "        if cv:\n",
    "            cv_size_batches, cv_limits_it = [], []\n",
    "            limit_iter, size_batch = 0, 0\n",
    "            \n",
    "            for i, ratio in enumerate(cv_ratios):\n",
    "                \n",
    "                if i < len(cv_ratios)-1:\n",
    "                    limit_iter += round(ratio*num_iters)\n",
    "                    size_batch += round(ratio*len(labels_train))\n",
    "                else:\n",
    "                    limit_iter = num_iters\n",
    "                    size_batch = len(labels_train)\n",
    "                \n",
    "                cv_limits_it.append(limit_iter)\n",
    "                cv_size_batches.append(size_batch)\n",
    "            \n",
    "            index_size_batch = np.argmax(it < np.array(cv_limits_it)) # This gives you the first occurrence where the condition is met\n",
    "            cv_size_batch = cv_size_batches[index_size_batch]\n",
    "            \n",
    "            gs_train_batch = gs_train[:cv_size_batch]\n",
    "            labels_train_batch = labels_train[:cv_size_batch]\n",
    "        \n",
    "        else:\n",
    "            batch_index = np.random.default_rng().choice(len(labels_train), size=batch_size, replace=False)\n",
    "            \n",
    "            gs_train_batch = gs_train[batch_index]\n",
    "            labels_train_batch = labels_train[batch_index]\n",
    "\n",
    "            \n",
    "        # Update the weights by one optimizer step\n",
    "        if num_cpus_batch == 0:\n",
    "            w, b, _, _, _ = opt.step(loss, w, b, gs_train_batch, labels_train_batch, 0)\n",
    "\n",
    "        else:\n",
    "            args = [[single_loss, w, b, gs_train_batch[j], labels_train_batch[j]] for j in range(len(labels_train_batch))]\n",
    "\n",
    "            with mp.Pool(num_cpus_batch) as pool:\n",
    "                w, b, _, _ = zip(*pool.starmap(opt.step, args))\n",
    "\n",
    "            w = sum(w)/len(w)\n",
    "            b = sum(b)/len(b)\n",
    "\n",
    "        weights.append(w)\n",
    "        bias.append(b)\n",
    "\n",
    "        # Compute predictions and accuracy on train and validation set\n",
    "        pred_train = pred(w, b, gs_train, num_cpus_train)\n",
    "        if with_val:\n",
    "            pred_val = pred(w, b, gs_val, num_cpus_val) if len(labels_val) > 0 else None\n",
    "        else:\n",
    "            pred_val = np.array([0]*len(labels_val))\n",
    "        \n",
    "        acc_train = acc(pred_train, labels_train)\n",
    "        if with_val:\n",
    "            acc_val = acc(pred_val, labels_val) if len(labels_val) > 0 else 0\n",
    "        else:\n",
    "            acc_val = 0\n",
    "        \n",
    "        # Save prediction for later plotting\n",
    "        pred_train_arr.append(pred_train)\n",
    "        pred_val_arr.append(pred_val)\n",
    "        acc_train_arr.append(acc_train)\n",
    "        acc_val_arr.append(acc_val)\n",
    "\n",
    "        l = loss(w, b, gs_train, labels_train, num_cpus_train)\n",
    "        losses.append(l)\n",
    "    \n",
    "    return weights, bias, losses, pred_train_arr, pred_val_arr, acc_train_arr, acc_val_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max train / Last run\n",
      "---------------------------------------------------\n",
      "  CV   | Run |   Iter    |Acc train|Acc val| Time  \n",
      "---------------------------------------------------\n",
      " False |   0 | 1798/1999 |  71/71  | 0/0 | 4445\n",
      " False |   1 |  690/1999 |  74/72  | 0/0 | 4823\n",
      " False |   2 |  507/1999 |  81/74  | 0/0 | 4704\n"
     ]
    }
   ],
   "source": [
    "time_now = datetime.now(pytz.timezone('Europe/Andorra')).strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "\n",
    "folder_name = f\"Results/{nqubits}q - {num_iters:} iters/\"\n",
    "if not os.path.isdir(f'{folder_name}'):\n",
    "    os.makedirs(f'{folder_name}')\n",
    "\n",
    "save_hyperparameters(time_now, folder_name, filename=\"\")\n",
    "\n",
    "\n",
    "print(\"Max train / Last run\")\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"  CV   | Run |   Iter    |Acc train|Acc val| Time  \")\n",
    "print(\"---------------------------------------------------\")\n",
    "\n",
    "\n",
    "for run in range (num_runs):\n",
    "    \n",
    "    # -------------------------------------------------------------- #\n",
    "    # ------------------- Generate ground states ------------------- #\n",
    "    # -------------------------------------------------------------- #\n",
    "    \n",
    "    gs_train, labels_train, j_train = generate_gs(train_size, uniform_train, epsilon_train, num_cpus_train)\n",
    "    gs_val, labels_val, j_val = generate_gs(val_size, uniform_val, epsilon_val, num_cpus_val)\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------ #\n",
    "    # ------------------- Train the QCNN ------------------- #\n",
    "    # ------------------------------------------------------ #\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    weights, \\\n",
    "    bias, \\\n",
    "    losses, \\\n",
    "    pred_train_arr, \\\n",
    "    pred_val_arr, \\\n",
    "    acc_train_arr, \\\n",
    "    acc_val_arr = train_qcnn(gs_train,\n",
    "                             gs_val,\n",
    "                             labels_train,\n",
    "                             labels_val,\n",
    "                             cv=False\n",
    "                            )\n",
    "    \n",
    "    run_time = time.time()-start_time\n",
    "    \n",
    "    \n",
    "    # --------------------------------------------------------- #\n",
    "    # ------------------- Save calculations ------------------- #\n",
    "    # --------------------------------------------------------- #\n",
    "    \n",
    "    save_data_2(time_now,\n",
    "                folder_name,\n",
    "                file_name_no_cv,\n",
    "                run,\n",
    "                weights,\n",
    "                bias,\n",
    "                losses,\n",
    "                j_train,\n",
    "                j_val,\n",
    "                pred_train_arr,\n",
    "                pred_val_arr,\n",
    "                acc_train_arr,\n",
    "                acc_val_arr,\n",
    "                run_time,\n",
    "                cv=False\n",
    "               )\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------- #\n",
    "    # ------------------------ QCNN with Curriculum ------------------------ #\n",
    "    # ---------------------------------------------------------------------- #\n",
    "    if with_cv:\n",
    "        # --------------------------------------------------------------------------------- #\n",
    "        # ------------------------ Sort training gs by their score ------------------------ #\n",
    "        # --------------------------------------------------------------------------------- #\n",
    "        \n",
    "        score_it = num_iters-1 # num_iters-1 or it_max\n",
    "        scores = [single_loss(weights[score_it], bias[score_it], gs_train[i], labels_train[i]) for i in range(len(labels_train))]\n",
    "        \n",
    "        table = {}\n",
    "        table[\"gs_train\"] = gs_train.tolist()\n",
    "        table[\"labels_train\"] = labels_train\n",
    "        table[\"j_train\"] = j_train.tolist()\n",
    "        table[\"scores\"] = scores\n",
    "\n",
    "        table = pd.DataFrame(table)\n",
    "        table.sort_values(by=[\"scores\"], inplace=True)\n",
    "\n",
    "        cv_gs_train = np.array(list(table[\"gs_train\"]))\n",
    "        cv_labels_train = np.array(list(table[\"labels_train\"]))\n",
    "        cv_j_train = np.array(list(table[\"j_train\"]))\n",
    "        \n",
    "        \n",
    "        # ---------------------------------------------------------------------------- #\n",
    "        # ------------------------ Train QCNN with Curriculum ------------------------ #\n",
    "        # ---------------------------------------------------------------------------- #\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        weights, \\\n",
    "        bias, \\\n",
    "        losses, \\\n",
    "        pred_train_arr, \\\n",
    "        pred_val_arr, \\\n",
    "        acc_train_arr, \\\n",
    "        acc_val_arr = train_qcnn(cv_gs_train,\n",
    "                                 gs_val,\n",
    "                                 cv_labels_train,\n",
    "                                 labels_val,\n",
    "                                 cv=True\n",
    "                                )\n",
    "        \n",
    "        run_time = time.time()-start_time\n",
    "        \n",
    "        \n",
    "        # --------------------------------------------------------- #\n",
    "        # ------------------- Save calculations ------------------- #\n",
    "        # --------------------------------------------------------- #\n",
    "        save_data_2(time_now,\n",
    "                    folder_name,\n",
    "                    file_name_no_cv,\n",
    "                    run,\n",
    "                    weights,\n",
    "                    bias,\n",
    "                    losses,\n",
    "                    j_train,\n",
    "                    j_val,\n",
    "                    pred_train_arr,\n",
    "                    pred_val_arr,\n",
    "                    acc_train_arr,\n",
    "                    acc_val_arr,\n",
    "                    run_time,\n",
    "                    cv=True\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### # Copy data from file\n",
    "\n",
    "# run_max = np.argmax(np.array(total_acc_train_max))\n",
    "# it_max_run_max = total_it_max[run_max]\n",
    "\n",
    "# avg_acc_train_max = sum(total_acc_train_max)/len(total_acc_train_max)\n",
    "# avg_acc_val_max = sum(total_acc_val_max)/len(total_acc_val_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(mp.cpu_count()) #256\n",
    "\n",
    "gs = gs_train\n",
    "lab = labels_train\n",
    "\n",
    "for i in range(10, 120, 10):\n",
    "    num_cpus_test = i\n",
    "    start_time = time.time()\n",
    "    loss(weights[0], bias[0], gs, lab, num_cpus_test)\n",
    "    print(f\"{i} --- {(time.time() - start_time):.2f} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define regions coordinates\n",
    "x01, y01 = region01e_coords[:,0], region01e_coords[:,1]\n",
    "x02, y02 = region02e_coords[:,0], region02e_coords[:,1]\n",
    "x1, y1 = region1e_coords[:,0], region1e_coords[:,1]\n",
    "x2, y2 = region2e_coords[:,0], region2e_coords[:,1]\n",
    "x3, y3 = region3e_coords[:,0], region3e_coords[:,1]\n",
    "\n",
    "# put the regions into the plot\n",
    "plt.fill(x01, y01, facecolor='lightskyblue')    # class 0\n",
    "plt.fill(x02, y02, facecolor='lightskyblue')    # class 0\n",
    "plt.fill(x1, y1, facecolor='sandybrown')        # class 1\n",
    "plt.fill(x2, y2, facecolor='salmon')            # class 2\n",
    "plt.fill(x3, y3, facecolor='lightgreen')        # class 3\n",
    "\n",
    "plt.axis('square')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
